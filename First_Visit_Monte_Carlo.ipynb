{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarun004545/Reinforcement-Learning/blob/main/First_Visit_Monte_Carlo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xuf8kBbES01D"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from collections import defaultdict\n",
        "from functools import partial\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Blackjack-v1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvhauDx_S5aV",
        "outputId": "503498e5-2950-4ee7-d6a8-cbe405d6bbbd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_policy(observation):\n",
        "    score, dealer_score, usable_ace = observation\n",
        "    return 0 if score >= 20 else 1"
      ],
      "metadata": {
        "id": "mxkducFBS7aV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_episode(policy, env):\n",
        "    \n",
        "    # we initialize the list for storing states, actions, and rewards\n",
        "    states, actions, rewards = [], [], []\n",
        "    \n",
        "    # Initialize the gym environment\n",
        "    observation = env.reset()\n",
        "    \n",
        "    while True:\n",
        "        \n",
        "        # append the states to the states list\n",
        "        states.append(observation)\n",
        "        \n",
        "        # now, we select an action using our sample_policy function and append the action to actions list\n",
        "         \n",
        "        action = sample_policy(observation)\n",
        "        actions.append(action)\n",
        "        \n",
        "        # We perform the action in the environment according to our sample_policy, move to the next state \n",
        "        # and receive reward\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        rewards.append(reward)\n",
        "        \n",
        "        # Break if the state is a terminal state\n",
        "        if done:\n",
        "             break\n",
        "                \n",
        "    return states, actions, rewards"
      ],
      "metadata": {
        "id": "PK8-yoGES7d5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def first_visit_mc_prediction(policy, env, n_episodes):\n",
        "    \n",
        "    # First, we initialize the empty value table as a dictionary for storing the values of each state\n",
        "    value_table = defaultdict(float)\n",
        "    N = defaultdict(int)\n",
        "\n",
        "    \n",
        "    for _ in range(n_episodes):\n",
        "        \n",
        "        # Next, we generate the epsiode and store the states and rewards\n",
        "        states, _, rewards = generate_episode(policy, env)\n",
        "        returns = 0\n",
        "        \n",
        "        # Then for each step, we store the rewards to a variable R and states to S, and we calculate\n",
        "        # returns as a sum of rewards\n",
        "        \n",
        "        for t in range(len(states) - 1, -1, -1):\n",
        "            R = rewards[t]\n",
        "            S = states[t]\n",
        "            \n",
        "            returns += R\n",
        "            \n",
        "            # Now to perform first visit MC, we check if the episode is visited for the first time, if yes,\n",
        "            # we simply take the average of returns and assign the value of the state as an average of returns\n",
        "            \n",
        "            if S not in states[:t]:\n",
        "                N[S] += 1\n",
        "                value_table[S] += (returns - value_table[S]) / N[S]\n",
        "    \n",
        "    return value_table"
      ],
      "metadata": {
        "id": "HvRn_YXNS7hV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value = first_visit_mc_prediction(sample_policy, env, n_episodes=500000)"
      ],
      "metadata": {
        "id": "R2TutoY9S7ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(value.popitem())"
      ],
      "metadata": {
        "id": "TD0v7g9nTD-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_blackjack(V, ax1, ax2):\n",
        "    player_sum = np.arange(12, 21 + 1)\n",
        "    dealer_show = np.arange(1, 10 + 1)\n",
        "    usable_ace = np.array([False, True])\n",
        "    state_values = np.zeros((len(player_sum), len(dealer_show), len(usable_ace)))\n",
        "\n",
        "    for i, player in enumerate(player_sum):\n",
        "        for j, dealer in enumerate(dealer_show):\n",
        "            for k, ace in enumerate(usable_ace):\n",
        "                state_values[i, j, k] = V[player, dealer, ace]\n",
        "    \n",
        "    X, Y = np.meshgrid(player_sum, dealer_show)\n",
        " \n",
        "    ax1.plot_wireframe(X, Y, state_values[:, :, 0])\n",
        "    ax2.plot_wireframe(X, Y, state_values[:, :, 1])\n",
        " \n",
        "    for ax in ax1, ax2:\n",
        "        ax.set_zlim(-1, 1)\n",
        "        ax.set_ylabel('player sum')\n",
        "        ax.set_xlabel('dealer showing')\n",
        "        ax.set_zlabel('state-value')"
      ],
      "metadata": {
        "id": "pFeW8UFMTECy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = pyplot.subplots(nrows=2, figsize=(5, 8),\n",
        "subplot_kw={'projection': '3d'})\n",
        "axes[0].set_title('value function without usable ace')\n",
        "axes[1].set_title('value function with usable ace')\n",
        "plot_blackjack(value, axes[0], axes[1])"
      ],
      "metadata": {
        "id": "15vD4Ie4TEHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9atYBOkOTEM_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}